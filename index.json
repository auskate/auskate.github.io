[{
    "title": "Bayesian divergence point analysis",
    "date": "",
    "description": "",
    "body": "\rThe visual world is a useful paradigm in psycholinguistics for tracking people’s eye fixations as they listen to sentences containing some kind of experimental manipulation. A common question is whether the experimental manipulation makes people fixate on a target object earlier in one condition versus another. To answer this, we need to decide when in each condition people start looking at the target more than other objects. But how do we compare these timepoints between conditions?1 We came up with a bootstrapping method for doing this in Stone, Lago \u0026amp; Schad, 2020. However, bootstrapping makes some weird assumptions about data, so here we try to address these by adding Bayesian principles to our bootstrapping method. This was an approach developed with João Veríssimo, Daniel Schad, and Sol Lago, and we apply it in this paper.\nThe bootstrap procedure\rA full workthrough of our bootstrap procedure can be found in Stone et al., 2020, but for a quick overview here, we use example data from an experiment on using syntactic gender to predict an upcoming noun in German. Participants heard sentences like “Klicke auf seinen blauen…” (click on his.MASC blue.MASC …), while looking at two blue objects on a screen. Only one of the objects matched the gender marking of the pronoun and adjective. There were two experimental conditions: in the match condition, the target object and the object’s owner matched in gender, e.g. seinen Knopf (his button.MASC). In the mismatch condition, the target object and the object’s owner mismatched in gender, e.g. ihren Knopf (her button.MASC).\nBecause of the gender cue on the possessive and adjective (e.g. -en suffix), we expected participants to predict the target object and look preferentially at it before they heard it’s name. What we really wanted to know though was whether predictive looks would be delayed in the mismatch condition where there was a conflicting gender cue, even though the conflicting cue was syntactically irrelevant (i.e. the object’s owner being masculine or feminine has no bearing on what the upcoming object might be). You can already see below that the onset of when participants looked preferentially at the target appears to be later in the mismatch condition:\nBut is this “prediction onset” in the mismatch condition significantly later than the match condition? We can find out using our bootstrapping procedure, which has the following steps:\nConduct a statistical test of fixations between the target and competitor at each timepoint in each condition (similar to a permutation test, e.g. Groppe, Urbach \u0026amp; Kutas, 2011; Maris \u0026amp; Oostenveldt, 2007; Barr, Jackson \u0026amp; Phillips, 2014),\rDecide on an alpha (usually 0.05) and find the first significant test statistic in a run of 5 consecutive significant test statistics (Sheridan \u0026amp; Reingold, 2012; Reingold \u0026amp; Sheridan, 2014 take a similar approach).2 This was our divergence point for each condition, which we consider the onset of evidence for predictions,\rResample the data 2000 times with replacement and repeat steps 1-2 after each resample.\r\rThe procedure thus has 3 distinct components:\na set of statistical tests comparing fixations to the target vs. competitor,\ra criterion for deciding where the onset is, and\ra way to generate a distribution of these onsets (resampling).\r\rThe unique contribution of our procedure versus existing methods was iii): we estimate the sampling distribution of an onset in each condition, which we can then use to statistically compare onsets between conditions.\nThe procedure yields two bootstrap distributions: one distribution each of onsets for the match and mismatch conditions respectively (see below). We take the mean and the 95th percentile confidence interval (CI) of the match/mismatch distributions as an estimate of the prediction onset for each condition and its temporal variability:\nBy subtracting the match from the mismatch distribution, we obtain a distribution of differences between conditions. We take the mean and the 95th percentile CI of this difference distribution as the estimated difference in prediction onset. We can decide whether the difference is different from zero by looking at whether the 95th percentile CI of the difference distribution contains zero. Since it does not, we can conclude that the difference between conditions is not zero. Moreover, since all values in the distribution are positive, we can conclude that predictions were slower in the mismatch condition:\nLimitations of the bootstrap procedure\rThe bootstrap makes some weird assumptions (Bååth, 2018):\n\rAny values not seen in the dataset are impossible\rEach value in the observed data has an equal probability of occurring every time the experiment is run\rWe have no prior knowledge about when in time predictions might onset\r\rFurthermore, while the procedure allows us to conclude that there is a significant difference between match and mismatch onsets, it does not allow us to quantify how much evidence we have for this finding.\n\r\rAdding Bayesian principles\rBayesian inference estimates our certainty about an event based on our prior knowledge about the probability of that event and new data. In our case, the event is prediction onsets. We can estimate our certainty about prediction onsets via Bayes’ theorem, which estimates a posterior probability distribution using two components: priors to encode our expectations about when the onset could be, and data to inform posterior inference via a likelihood function3.\nWe start with the priors: we reasoned that predictiton onsets could only arise in the 1600 ms time window between the onset of the pronoun and the onset of the noun (adding 200 ms for saccade planning). We therefore specified a normal distribution centered in the middle of this critical window, with a 95% probability of the onset falling between 200 and 1800 ms: \\(N(1000,400)\\). Our prior distributions for the match and mismatch conditions therefore looked like this:\nNext, we needed a likelihood function. This involved two steps: First, we used the distribution of onsets estimated via the original bootstrap procedure to approximate a likelihood. Second, we used a normal distribution to approximate the likelihood function (i.e., Laplace approximation) under the assumption of the central limit theorem that the distribution underlying the bootstrap distribution was approximately normal4. This gave us a likelihood distribution with a mean and standard deviation derived from the bootstrap data:\nFinally, we can now find the posterior distribution as the product of the prior and the likelihood. Because the prior and the likelihood are normal distributions, the posteriors for each condition can be derived analytically as the product of two Gaussian probability density functions, where\n\r\\(N(\\mu_{prior},\\sigma_{prior})\\) is the prior,\r\\(N(\\mu_{lik},\\sigma_{lik})\\) is the likelihood,\r\\(\\mu_{posterior}\\) is the mean of the posterior, and\r\\(\\sigma_{posterior}\\) is the standard deviation of \\(\\mu_{posterior}\\)\r\rThe posterior distribution of the onset in the match condition is therefore:\n\\[ \\begin{aligned}\r\\mu_{posterior} \u0026amp;= \\frac{\\mu_{prior} \\sigma_{lik}^2 + \\mu_{lik} \\sigma_{prior}^2}{\\sigma_{lik}^2 + \\sigma_{prior}^2} \\\\\r\u0026amp;= \\frac{1000 \\cdot 22^2 + 367 \\cdot 400^2}{22^2 + 400^2} \\\\ \u0026amp;= 369 ms\r\\end{aligned}\r\\]\n\\[ \\begin{aligned}\r\\sigma_{posterior} \u0026amp;= \\sqrt\\frac{\\sigma_{prior}^2 \\sigma_{lik}^2}{\\sigma_{prior}^2 + \\sigma_{lik}^2} \\\\ \u0026amp;= \\sqrt\\frac{400^2 \\cdot 22^2}{400^2 + 22^2} \\\\ \u0026amp;= 22 ms \\end{aligned}\r\\]\nVia the same calculation, the posterior for the mismatch onset is 705 ms (SD = 9 ms). We can add these posteriors to our plots and see that they resemble the bootstrap data:\nThis is because our prior was relatively uninformative, and so the posteriors are informed more strongly by the bootstrap data than the prior. If we had defined a strongly informative prior to say we were very certain that the prediction onsets would be 1000 ms, e.g. \\(N(1000, 10)\\), then the posterior would be pulled toward the prior (not completely, as it’s still being informed by the data):\nHow do the Bayesian onsets (red, uninformative priors) compare with the onsets from the original bootstrap procedure (black)? Quite well:\nNow, how to decide whether the mismatch condition was slower? Because the match and mismatch posteriors are normal distributions, we can find the posterior of their difference as the difference of two normal distributions:\n\\[ \\begin{aligned}\r\\mu_{posterior_{difference}} \u0026amp;= \\mu_{posterior_{mismatch}} - \\mu_{posterior_{match}} \\\\\r\u0026amp;= 705 - 369 \\\\\r\u0026amp;= 336 ms\r\\end{aligned}\r\\]\n\\[ \\begin{aligned}\r\\sigma_{posterior_{difference}} \u0026amp;= \\sqrt(\\sigma^2_{posterior_{mismatch}} + \\sigma^2_{posterior_{match}}) \\\\\r\u0026amp;= \\sqrt(9^2 + 22^2) \\\\\r\u0026amp;= 24ms\r\\end{aligned}\r\\]\nWe now have a posterior estimate that predictions in the mismatch condition were 336 ms slower than in the match condition, with a 95% credible interval of 288–384 ms. This posterior overlaps quite well with the original bootstrap difference distribution, which estimated a difference of 338 ms with a 95th percentile interval of 300–380ms.\nQuantifying evidence with a Bayes factor\rHow much evidence do we have that the distribution of match/mismatch prediction onset differences is not zero? Since the likelihood is not normalised, we can use the Savage-Dickey method to compute a Bayes factor (Dickey \u0026amp; Lientz, 1970; Wagenmakers et al., 2010). This method finds the ratio of prior to posterior density at some point value (e.g. zero). In other words, we are trying to find how much seeing the data has changed our prior belief that the true difference between conditions is zero:\nWe compute the ratio via the equation below, where\n\r\\(\\theta\\) is the point at which we want to compare densities (e.g. zero),\r\\(H_0\\) is our prior distribution,\r\\(H_1\\) is our posterior distribution, and\r\\(D\\) is the data:\r\r\\[ BF_{10} = \\frac{p(D|H_1)}{p(D|H_0)}= \\frac{p(\\theta = 0|H_1)}{p(\\theta = 0|D,H_1)} \\]\nOr in R form:\n# define the null hypothesis\rnull_hypothesis \u0026lt;- 0\r# find the density of the prior at the null hypothesis\rdensity_prior_null \u0026lt;- dnorm(null_hypothesis, difference_prior_mean, difference_prior_sd)\r# find the density of the posterior at the null hypothesis\rdensity_posterior_null \u0026lt;- dnorm(null_hypothesis, difference_posterior_mean, difference_posterior_sd)\rBecause the posterior density at zero is less than the prior density (in fact, the posterior density is almost zero), the Bayes factor strongly favours the alternative hypothesis that there is a match/mismatch difference:\n# use Savage-Dickey ratio to compute the Bayes factor\r(BF10 \u0026lt;- 1/(density_posterior_null/density_prior_null))\r## [1] 7.130063e+43\r\r\rConclusions\rUsing our Bayesian divergence point analysis, we find evidence that prediction onsets were slower when there were two conflicting gender cues. The posterior estimate of the “mismatch effect” size was 336 ms, with a 95% credible interval of 288–384 ms. But our existing bootstrap procedure already led us to the same conclusion, so what was the advantage of adding Bayesian principles?\nThe main advantage is one of interpretation. The mean of the posterior is now part of a continuous probability distribution that gives us information about the probability of non-observed values in the data. The 95% credible interval can be interpreted as a range of possible between-condition differences in which the true size of the difference should lie with 95% probability, given the data and the analysis. This interpretation is more intuitive than the percentile confidence interval from the existing bootstrap method, which just told us were 95% of the resampled data lay. Finally, because the Bayesian method uses priors, we can use our posterior estimates as information about plausible effect sizes to inform the priors of future experiments.\n\r\rThere are actually several existing methods for answering temporal questions about visual world data, including cluster permutation, BDOTS, and GAMMs. We summarise these in Stone et al., 2020 and describe why they weren’t able to answer the specific question we had for the above experiment, which was whether one condition’s onset was significantly faster than another.↩︎\n\rDepending on your experimental manipulation (e.g. how “big” or sustained you expect the effect on looks to be) and how you’ve set up your data (e.g. binned, unbinned, eye tracker sampling rate), your criterion for the number of consecutive significant tests may differ.↩︎\n\rOther possibilities for adding Bayesian principles to our procedure could have been to use the Bayesian Bootstrap (Rubin, 1981), implemented in R in bayesboot (Bååth, 2018). Unfortunately, bayesboot didn’t suit our particular bootstrapping method, partly because it doesn’t take a multi-step function like ours (see steps 1-3 above), but also because it doesn’t take more informative priors—at least not currently. Alternatively, we could have fit e.g. GLMMs in brms (Bürkner, 2018) to test between fixation proportions at each timepoint (step 1). But this would only apply Bayesian inference to estimating the magnitude of the difference in fixations between target and competitor at each timepoint, when what we really want is to incorporate a prior about the temporal variability of the onset.↩︎\n\rUsing a normal distribution for the likelihood assumes that, with sufficient observations and bootstrap samples, the bootstrap distribution will approach a normal distribution in line with the central limit theorem. However, this is not always the case, as can be seen with our data. An alternative way to define the likelihood would be to use a kernel density estimator instead; we present this approach in the appendices of this paper.↩︎\n\r\r\r",
    "ref": "/post/2021-04-08-bayesian-divergence-point-analysis/"
  },{
    "title": "",
    "date": "",
    "description": "",
    "body": "Please come back later!\n",
    "ref": "/drafts/comebacklater/"
  },{
    "title": "About me",
    "date": "",
    "description": "",
    "body": "I am a psycholinguist in the Vasishth Lab and the Cognitive Neuroscience Lab at the University of Potsdam, Germany. My interest is in probabilistic sentence processing and what this tells us about how words are stored and accessed in the brain. More generally, I am interested in how different fields such as neuroscience, computer science and linguistics can inform each other to build comprehensive theories of human cognition.\nCV PhD Cognitive Science (University of Potsdam, Germany)\nMSc Experimental Linguistics (University of Potsdam, Germany)\nBA Linguistics (University of Melbourne, Australia)\nB.Behavioural Neuroscience (Monash University, Australia)\n",
    "ref": "/drafts/about/"
  },{
    "title": "Publications",
    "date": "",
    "description": "",
    "body": " Papers Stone, K., Veríssimo, J., Schad, D., Oltrogge, E., Vasishth, S., \u0026amp; Lago, S. (in revision) The interaction of grammatically distinct agreement dependencies in predictive processing. [preprint]\nStone, K., Nicenboim, B., Vasishth, S., \u0026amp; Rösler, F. (Stage 1 RR accepted) Understanding the effects of constraint and predictability in ERP. Neurobiology of Language.\nStone, K., von der Malsburg, T., \u0026amp; Vasishth, S. (2020) The effect of decay and lexical uncertainty on processing long-distance dependencies in reading. PeerJ. [paper] [data/code]\nStone, K., Lago, S., \u0026amp; Schad, D. (2020) Divergence point analyses of visual world data: applications to bilingual research. Bilingualism: Language and Cognition. [paper] [data/code]\n Conference contributions Lago, S., Oltrogge, E., \u0026amp; Stone, K. (2020) Are native and non-native speakers differentially sensitive to agreement attraction? Poster presented at AMLaP in Potsdam, Germany, September 4, 2020. [poster]\nStone, K., Oltrogge, E., Vasishth, S. \u0026amp; Lago, S. (2020) The real-time application of grammatical constraints to prediction: Timecourse evidence from eye tracking. Talk presented at CUNY in Amherst, USA, March 20, 2020. [slides]\nStone, K., Vasishth, S. \u0026amp; von der Malsburg, T. (2020) Contextual constraint and the frontal post-N400 positivity: A large-sample, pre-registered ERP study. Poster presented at CUNY in Amherst, USA, March 20, 2020. [poster]\nStone, K. \u0026amp; Lago, S. (2020) Individual variability in the timecourse of predictions. Talk presented at DGfS in Hamburg, Germany, March 4, 2020. [slides]\nStone, K. \u0026amp; Lago, S. (2019) L1 influence in L2 gender predictions: A visual world study. Poster presented at ESCoP in Tenerife, Spain, September 26, 2019. [poster]\nStone, K., Vasishth, S. \u0026amp; von der Malsburg, T. (2019) Comprehenders generate long-distance predictions during reading: ERP evidence from verb particle constructions. Talk presented at PIPP in Reykjavik, Iceland, June 19-20, 2019. [slides]\nStone, K., Vasishth, S. \u0026amp; von der Malsburg, T. (2019) ERP evidence for long-distance lexical predictions in German particle verb constructions. Poster presented at CUNY in Boulder, USA, March 29-31, 2019. [poster]\nStone, K., Vasishth, S. \u0026amp; von der Malsburg, T. (2018) Expectations and prediction in sentence processing: German particle verbs as a test case. Poster presented at CUNY in Davis, USA, March 15-17, 2018. [poster]\nStone, K., Mertzen, D. \u0026amp; Vasishth, S. (2017) Verb particle predictability determines the facilitation effect of pre-particle material. Poster presented at Architectures and Mechanisms of Language Processing (AMLaP) in Lancaster, UK, September 7-9, 2017. [poster]\nStone, K., Jessen, A. \u0026amp; Clahsen, H. (2016) Delayed versus immediate production: ERP studies of verb morphology. Poster presented at the International Workshop on Language Production in San Diego, USA, July 25-27, 2016.\n",
    "ref": "/publications/"
  },{
    "title": "About me",
    "date": "",
    "description": "",
    "body": " I am a psycholinguist in the Vasishth Lab and the Cognitive Neuroscience Lab at the University of Potsdam, Germany. My interest is in probabilistic sentence processing and what this tells us about how words are stored and accessed in the brain. More generally, I am interested in how different fields such as neuroscience, computer science and linguistics can inform each other to build comprehensive theories of human cognition.\nCV PhD Cognitive Science (University of Potsdam, Germany)\nMSc Experimental Linguistics (University of Potsdam, Germany)\nBA Linguistics (University of Melbourne, Australia)\nB.Behavioural Neuroscience (Monash University, Australia)\n",
    "ref": "/first/"
  },{
    "title": "Kate Stone",
    "date": "",
    "description": "",
    "body": "\r\n\r\n\r\n\r\n",
    "ref": "/drafts/2020-12-01-r-rmarkdown/"
  }]
